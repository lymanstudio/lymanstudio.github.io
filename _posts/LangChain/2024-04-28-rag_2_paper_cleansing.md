```
layout: single
title:  "RAG ì‹¤ìŠµ 2. PDFíŒŒì¼ê³¼ ëŒ€í™”í•˜ê¸°(ì¤‘: ì–¸ì–´ëª¨ë¸ì„ ì‚¬ìš©í•œ Document ì •ì œ, ì˜ë¯¸ì ìœ¼ë¡œ Chunkingí•˜ê¸°)"
classes: wide
categories: LangChain
tags: [LangChain, RAG]
```

[ì´ì „ í¬ìŠ¤íŠ¸](https://lymanstudio.github.io/langchain/rag_1_loader_vectorstore/)ì—ì„œ ìš°ë¦¬ëŠ” Document Loadingë¶€í„° ë²¡í„° ìŠ¤í† ì–´ êµ¬ì„±ê¹Œì§€ ì „ë°˜ì ì¸ ê³¼ì •ì„ ê° ë‹¨ê³„ë³„ íŠœë‹ ì—†ì´ ë¹ ë¥´ê²Œ í›‘ì–´ë³´ì•˜ë‹¤. í•˜ì§€ë§Œ ì¤‘ê°„ì— ë¬¸ì„œë¥¼ ì •ì œí•˜ì§€ë„ ì•Šì•„ ë¬¸ì„œì— ìˆëŠ” í…ìŠ¤íŠ¸ë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©í–ˆìœ¼ë©° í•œ í˜ì´ì§€ì˜ ì „ì²´ ë‚´ìš©ì´ í•˜ë‚˜ì˜  Documentì— ë“¤ì–´ê°€ ì°¨í›„ ë¬¸ì„œë“¤ì„ ê²€ìƒ‰ì— ì‚¬ìš©í•˜ê¸°ì— ìš©ì´í•˜ì§€ë„ ì•Šì€ ìƒíƒœì´ë‹¤.



ì´ë²ˆ í¬ìŠ¤íŠ¸ì—ì„  ìœ„ì˜ ë‚´ìš©ë“¤ì„ ì§‘ì¤‘ì ìœ¼ë¡œ ë‹¤ë¤„ë³¼ ê²ƒì´ë‹¤. 

* ìš°ì„  ì²«ë²ˆì§¸ë¡œ loadëœ Documentë“¤ì„ ì •ì œí•˜ëŠ” ê³¼ì •ì„ ì‚´í´ë³´ê³  ë‹¤ì‹œ  Documentë¡œ êµ¬ì„±í•œë‹¤. ì´ê³¼ì •ì—ì„œ ê°„ë‹¨í•œ LMì„ ì‚¬ìš©í•  ê²ƒì´ë‹¤.

* ë‘ë²ˆì§¸ë¡œ ì •ì œëœ Documentë“¤ì„ TextSpliterë¥¼ ì‚¬ìš©í•´ ì‘ì€ chunkë¡œ ìª¼ê°œì–´ì¤„ ê²ƒì´ë‹¤. ì´ ê³¼ì •ì—ì„œ ê° ë¬¸ì¥ ì‚¬ì´ì˜ ìœ ì‚¬ë„ë¥¼ êµ¬í•´ ê·¸ê²ƒì„ ê¸°ë°˜ìœ¼ë¡œ ë¬¸ì¥ë“¤ì˜ ë¶€ë¶„ ì§‘í•©ìœ¼ë¡œ ë‚˜ëˆ„ì–´ ì¤„ ì˜ˆì •ì´ë‹¤.

ì˜¤ëŠ˜ ë‹¤ë¤„ë³¼ ê³¼ì •ì€ ë‹¤ìŒê³¼ ê°™ë‹¤.

![rag_2](../../images/2024-04-28-rag_2_paper_cleansing/rag_2.jpg)

---



# Step 3. ì–¸ì–´ ëª¨ë¸ì„ í™œìš©í•œ ë¬¸ì„œ ì •ì œ

ë¬¸ì„œë¥¼ ë²¡í„° DBì— ì €ì¥í•˜ê¸° ì „ì— ìš°ì„  loadëœ ë¬¸ì„œì˜ ë³¸ë¬¸ì„ ì •ë¦¬í•  í•„ìš”ê°€ ìˆë‹¤. ìš°ë¦¬ê°€ ì˜ˆì œë¡œ ì“°ëŠ” ë…¼ë¬¸ì„ ìì„¸íˆ ë¶„í•´í•´ë³´ë©´ ë‹¤ë¥¸ í…ìŠ¤íŠ¸ë¡œ ëœ ê°„ê²°í•œ ë¬¸ì„œë“¤ê³¼ ë‹¤ë¥¸ ì—¬ëŸ¬ ê°€ì§€ íŠ¹ì§•ì´ ìˆë‹¤. 
1. ëª¨ë“  í˜ì´ì§€ ìƒ/í•˜ë‹¨ì— ì €ë„ ì •ë³´, í˜ì´ì§€ ë²ˆí˜¸ê°€ ìˆë‹¤.
2. ì¤‘ê°„ì— ë„í‘œë‚˜ ê·¸ë¦¼ì´ ìˆì„ ê²½ìš° ì„¤ëª… ìº¡ì…˜ì´ ë”°ë¼ ë¶™ëŠ”ë‹¤.
3. References, ì¦‰ ì°¸ê³  ë¬¸í—Œë“¤ì€ ì—¬ëŸ¬ citationìŠ¤íƒ€ì¼ë¡œ ë¬¸ìë“¤ì´ ë‹¨ìˆœíˆ ë‚˜ì—´ë¼ìˆìœ¼ë©° ê·¸ ìì²´ë§Œìœ¼ë¡œ í° ì˜ë¯¸ë¥¼ ê°€ì§€ê¸° í˜ë“¤ë‹¤.
4. ì´ˆë¡ì€ ë…¼ë¬¸ì˜ ì „ì²´ ë‚´ìš©ì„ ìš”ì•½í•˜ëŠ” ì•„ì£¼ ì¤‘ìš”í•œ ë¬¸ë‹¨ì´ë‹¤.

ìœ„ íŠ¹ì§•ë“¤ì„ ì°¸ê³  í•˜ì—¬ ë…¼ë¬¸ì˜ ë‚´ìš©ë“¤ì„ ê°€ë‹¤ë“¬ì„ í•„ìš”ê°€ ìˆë‹¤. íŠ¹íˆ ì €ë„ëª…ê³¼ ê°œì œëœ í˜¸ìˆ˜ ë“±ì˜ ë°˜ë³µë˜ëŠ” ì •ë³´ëŠ” í° ì˜ë¯¸ê°€ ì—†ìœ¼ë‚˜ ì´í›„ ì§„í–‰ë  textsplit ë‹¨ê³„ì—ì„œ ëª…ì‹œì ìœ¼ë¡œ ë¬´ì‹œí•˜ê¸´ í˜ë“œë¯€ë¡œ í•„ìˆ˜ì ìœ¼ë¡œ ì œê±°ë¼ì•¼ í•œë‹¤.

í•˜ì§€ë§Œ ëª…ì‹œì ì€ rule-based ì œê±° ë°©ë²•ì€ ì—†ìœ¼ë¯€ë¡œ LLMì˜ í˜ì„ ë¹Œë ¤ ë¬¸ì„œ cleansingì„ ì§„í–‰í•œë‹¤.

ë¬¸ì„œ cleansingì„ ì§„í–‰í•˜ê¸° ìœ„í•´ ìš°ì„  í”„ë¡¬í”„íŠ¸ë¥¼ ì‘ì„±í•´ì•¼ í•œë‹¤.
ëª¨ë¸ì—ê²Œ ë…¼ë¬¸ ì „ë¬¸ í¸ì§‘ìë¼ëŠ” roleì„ ì£¼ê³  ìœ„ì—ì„œ ë‚˜ì—´í•œ íŠ¹ì§•ì„ ê°ì•ˆí•´ì„œ ìì„¸í•˜ê²Œ ëª¨ë“  ë‚´ìš©ì„ ë‹´ì€ í¸ì§‘ë³¸ì„ ë‹¬ë¼ê³  ë§í•œë‹¤.
ë˜í•œ í˜ì´ì§€ê°€ ë‚˜ëˆ ì ¸ ìˆìœ¼ë¯€ë¡œ ì•ì„  í˜ì´ì§€ì˜ ê²°ê³¼ë¥¼ ë‹¤ìŒ í˜ì´ì§€ì˜ ìƒì„±ì— ì°¸ê³ í•˜ë„ë¡ êµ¬ì„±í–ˆë‹¤.

ì™„ì„±í•œ í”„ë¡¬í”„íŠ¸ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.


```python
from langchain.prompts import PromptTemplate

template = """
You are an editor who is expert on editing thesis papers into a rich and redundant-erased writings. Your job is to edit PAPER.
If the client gives you PAPER(a part of thesis paper) with PRV_PAGE(the summary of the previous page).
To make a edited version of PAPER, you have to keep the following rules.
1. Erase all the additional information that are not directly related to the idea and content of the paper, such as the name of journal, page numbers, so on.
In most case, those additional information is located in the first or the last part of PAPER. 
2. Erase all the reference/citation marks of numbers in the middle of PAPER.
3. Edit PAPER in a rich manner and should contains all the idea and content. Do not discard any content. 
4. It has to be related and successive to the content of PRV_PAGE. But should not repeatedly have the PRV_PAGE content.
5. Note that there are successive pages waiting to be edited, so the result should not be ended with the feeling that it is the last document.
6. Do not conclude at the end the current editing, unless PAPER only contains references(imply that current PAPER is the end of the thesis). 

## PRV_PAGE: {prv_page}

## PAPER: {content} 
"""
```

ë‹¤ìŒìœ¼ë¡œ ê°„ë‹¨í•˜ê²Œ LCEL êµ¬ë¬¸ìœ¼ë¡œ ë¬¸ì„œ cleansing chainì„ êµ¬ì„±í–ˆë‹¤. LLMëª¨ë¸ì€ gpt-3.5-turboë¥¼ ì‚¬ìš©í–ˆë‹¤.


```python
prompt = PromptTemplate.from_template(template)

from langchain_openai import ChatOpenAI
from langchain_core.output_parsers import StrOutputParser

model = ChatOpenAI(model='gpt-3.5-turbo')
cleansing_chain = prompt | model | StrOutputParser()
```

ìƒ˜í”Œë¡œ ì²«ë²ˆì§¸ í˜ì´ì§€ë¥¼ ìœ„ ëª¨ë¸ì— ì ìš©í•´ ê²°ê³¼ë¥¼ ì‚´í‘œë³´ì. ì²«ë²ˆì§¸ í˜ì´ì§€ì´ë¯€ë¡œ `PRV_PAGE`ëŠ” ë¹ˆ ìŠ¤íŠ¸ë§ì„ ë„£ì–´ì¤€ë‹¤.


```python
prv_page = """
"""
result = cleansing_chain.invoke({
    "prv_page" : prv_page,
    "content" : docs[0].page_content
})

print(result)
```

    The paper introduces a novel framework for classifying indoor elements and representing them in a vector format. Unlike traditional methods that use image-based learning to segment pixels, this framework converts the floor plan image into vector data and utilizes a graph neural network. The three-step process includes image pre-processing and vectorization, region adjacency graph conversion, and applying the graph neural network on the converted floor plan graphs.
    
    This approach is capable of identifying various indoor elements, including walls, doors, symbols, rooms, corridors, and even element shapes. Experimental results demonstrate that the framework achieves a 95% F1 score with scale and rotation invariance. Additionally, a new graph neural network model is proposed that considers the distance between nodes, a crucial aspect of spatial network data.
    
    Floor plans are essential drawings that depict the layout of a specific level in a building or structure, containing structural elements like walls, windows, doors, and spatial elements such as rooms and corridors. Digitizing floor plans poses challenges as they are primarily images without explicit object information. Therefore, feature extraction and analysis of indoor spatial data from floor plan images require pre-processing and analytical algorithms. While heuristic algorithms yield high accuracy, they are limited to specific drawing styles, prompting the adoption of machine learning-based approaches.
    
    Convolutional Neural Network-based methods have gained popularity due to their applicability to various floor plan styles, minimal pre-processing requirements, and robustness to noise. However, these methods struggle to capture the exact shape of indoor elements, necessitating additional post-processing steps to abstract neural network outputs. Despite this, abstracting the floor plan layout through machine learning may result in a loss of original element features, such as the representation of walls as line vectors instead of polygons.
    
    In conclusion, the proposed framework offers a promising solution for indoor element classification and representation in vector format, showcasing high accuracy and versatility in capturing diverse types of indoor elements. The integration of a graph neural network model that considers node distances enhances the analysis of spatial network data, presenting a valuable contribution to floor plan analysis and spatial data processing.



```python
prv_page = result
result = cleansing_chain.invoke({
    "prv_page" : prv_page,
    "content" : docs[1].page_content
})

print(result)
```

    In this study, we present a novel framework for identifying indoor elements within floor plans while preserving their original shape information. The framework involves a three-step process: first, the input floor plan image is vectorized to maintain the shape of indoor elements and minimize abstraction. Next, the polygon vector set is converted into a region adjacency graph, which is then input into a graph neural network (GNN) for node classification. The GNN analyzes inherent features and relationships between nodes to classify basic indoor elements (e.g., walls, windows, doors) and symbols, as well as spatial elements (e.g., rooms, corridors). This approach ensures that the shape and aerial features of elements are retained throughout the classification process.
    
    Furthermore, we introduce a new GNN model called the Distance-Weighted Graph Neural Network (DWGNN). This model considers the distance information between nodes, expressed through edge features in the spatial network. By assigning attention values to neighboring nodes based on their proximity to a target node, the DWGNN enhances the analysis of spatial network data. We evaluate the performance and expressiveness of this framework by applying it to two floor plan datasets and one data-augmented dataset.
    
    The following sections of the paper discuss the limitations of previous research on floor plan analysis, particularly in indoor element classification using rule-based methods and machine learning approaches. We then propose our framework for floor plan element classification via GNN, highlighting its advantages over traditional heuristic methods and its ability to maintain the shapes of elements across different drawing styles. Finally, we present the results of our analysis on three datasets and discuss potential issues and areas for further research. Our framework offers a promising solution for accurately classifying indoor elements in vector format, showcasing high accuracy and versatility in capturing diverse types of indoor elements.


ê²°ê³¼ê°€ ë§Œì¡± ìŠ¤ëŸ¬ìš°ë‹ˆ ì „ì²´ í˜ì´ì§€ì— ëŒ€í•´ ì´ ê³¼ì •ì„ ì§„í–‰í•˜ê³  ì´ë¥¼ í†µí•´ ìƒˆë¡œìš´ Documentë¥¼ ë§Œë“¤ì–´ì¤€ë‹¤.


```python
from langchain_core.documents import Document
from time import time

start = time()
result_docs = []
result_concat = ""
prv_page = ""
for i, doc in enumerate(docs):
    result = cleansing_chain.invoke({
        "prv_page" : prv_page,
        "content" : doc.page_content
    })

    result_docs.append(Document(page_content = result, paper_metadata = doc.metadata))
    result_concat += ("\n" + result)
    prv_page = result
    print(f"{i+ 1} ë²ˆì§¸ ë¬¸ì„œ ì™„ë£Œ({i + 1}/{len(docs)})")    

print(f"elapsed time : {time() - start} seconds")
```

    1 ë²ˆì§¸ ë¬¸ì„œ ì™„ë£Œ(1/17)
    2 ë²ˆì§¸ ë¬¸ì„œ ì™„ë£Œ(2/17)
    3 ë²ˆì§¸ ë¬¸ì„œ ì™„ë£Œ(3/17)
    4 ë²ˆì§¸ ë¬¸ì„œ ì™„ë£Œ(4/17)
    5 ë²ˆì§¸ ë¬¸ì„œ ì™„ë£Œ(5/17)
    6 ë²ˆì§¸ ë¬¸ì„œ ì™„ë£Œ(6/17)
    7 ë²ˆì§¸ ë¬¸ì„œ ì™„ë£Œ(7/17)
    8 ë²ˆì§¸ ë¬¸ì„œ ì™„ë£Œ(8/17)
    9 ë²ˆì§¸ ë¬¸ì„œ ì™„ë£Œ(9/17)
    10 ë²ˆì§¸ ë¬¸ì„œ ì™„ë£Œ(10/17)
    11 ë²ˆì§¸ ë¬¸ì„œ ì™„ë£Œ(11/17)
    12 ë²ˆì§¸ ë¬¸ì„œ ì™„ë£Œ(12/17)
    13 ë²ˆì§¸ ë¬¸ì„œ ì™„ë£Œ(13/17)
    14 ë²ˆì§¸ ë¬¸ì„œ ì™„ë£Œ(14/17)
    15 ë²ˆì§¸ ë¬¸ì„œ ì™„ë£Œ(15/17)
    16 ë²ˆì§¸ ë¬¸ì„œ ì™„ë£Œ(16/17)
    17 ë²ˆì§¸ ë¬¸ì„œ ì™„ë£Œ(17/17)
    elapsed time : 110.78608131408691 seconds



# Step 4. TextSpliterë¥¼ ì‚¬ìš©í•´ ë¬¸ì„œ Chunking í•˜ê¸°

TextSpliter í´ë˜ìŠ¤ëŠ” ì´ë¦„ì˜ ì˜ë¯¸ ê·¸ëŒ€ë¡œ í…ìŠ¤íŠ¸ë¥¼ ë¶„í• í•´ì£¼ëŠ” í´ë˜ìŠ¤ì´ë‹¤. ê¸°ë³¸ì ìœ¼ë¡œ LM ì´ ì–¸ì–´ëª¨ë¸ì´ê¸°ì— ì…ë ¥ì´ ë¬¸ìì—´ì¸ ê²½ìš°ê°€ ë§ê³  ì…ë ¥ìœ¼ë¡œ ì£¼ëŠ” ë°ì´í„°ë¥¼ ëª©ì ì— ë§ê²Œ ì˜ ë‚˜ëˆ ì£¼ëŠ” ê²ƒì´ í•„ìš”í•˜ë‹¤.
ìš°ë¦¬ëŠ” ì•ì„œ PDFíŒŒì¼ì„ ê°€ì ¸ì˜¬ ë•Œ ë¬¸ì„œ í˜ì´ì§€ ë‹¨ìœ„ë¡œ ì˜ë¼ Document ê°ì²´ë¥¼ ìƒì„±í–ˆì§€ë§Œ ì´ëŸ° ê²½ìš° ì‚¬ìš©ìê°€ ì›í•˜ëŠ” ê¸°ëŠ¥ê³¼ ìœ ì‚¬í•œ ë¬¸ì„œë¥¼ ê°€ì ¸ì˜¬ ë•Œ ìƒë‹¹íˆ ë§ì€ ìˆ˜ì˜ í…ìŠ¤íŠ¸ë¥¼ ë‹´ê³  ìˆëŠ” Documentë“¤ì´ ë°˜í™˜ë  ê²ƒì´ê¸°ì— ì…ë ¥ ê¸¸ì´ê°€ ê¸¸ì–´ì§ˆ ìˆ˜ ë°–ì— ì—†ìœ¼ë©° ë¬´ì—‡ë³´ë‹¤ ì‚¬ìš©ìê°€ ì›í•˜ëŠ” ë‚´ìš©ê³¼ ìƒê´€ ì—†ëŠ” ë¶ˆí•„ìš”í•œ ë‚´ìš©ë“¤ì„ ê°€ì§€ê³  ìˆì„ í™•ë¥ ë„ ë†’ë‹¤.

ìš°ë¦¬ê°€ ì›í•˜ëŠ” ì´ìƒì ì¸ DocumentëŠ” 
1. ì‚¬ìš©ìê°€ ì§ˆë¬¸/ìš”êµ¬í•˜ëŠ” ë‚´ìš©ê³¼ ì˜ë¯¸ë¡ ì ìœ¼ë¡œ ìœ ì‚¬í•œ ë‚´ìš©ì€ ìµœëŒ€í•œ ê°€ì§€ê³  ìˆì–´ì•¼ í•˜ê³ 
2. ê·¸ì™¸ì˜ ë¶€ê°€ì ì´ê±°ë‚˜ ê´€ë ¨ ì—†ëŠ” ë‚´ìš©ì€ ìµœëŒ€í•œ ê°€ì§€ê³  ìˆì§€ ì•Šì•„ì•¼ í•˜ë©°
3. ê·¸ ìˆ˜ê°€ ë„ˆë¬´ ë§ì§€ ì•Šì•„ì•¼ í•œë‹¤(ëª¨ë¸ ì…ë ¥ í¬ê¸°ì˜ ì œí•œì´ ìˆê±°ë‚˜ í† í° ìˆ˜ê°€ ë§ì•„ì§€ë©´ ê·¸ë§Œí¼ ë¹„ìš©/ì»´í“¨íŒ… ë¦¬ì†ŒìŠ¤ ì‚¬ìš© ë©´ì—ì„œ ë¹„íš¨ìœ¨ì ì´ê¸°ì—).

ë”°ë¼ì„œ ìš°ë¦¬ëŠ” TextSpliterë¥¼ ì‚¬ìš©í•´ ìš°ë¦¬ì˜ í…ìŠ¤íŠ¸ ìë£Œë¥¼ ìœ„ì˜ ì„¸ê°€ì§€ ê¸°ì¤€ì„ ë§Œì¡±í•˜ëŠ” Documentë¡œ ë¶„í• í•´ì•¼ í•œë‹¤.



## ê¸°ë³¸ì ì¸ spliter

ìš°ì„  ê¸°ë³¸ì ì¸ ë¶„í• ë¶€í„° ì•Œì•„ë³´ì. `CharacterTextSplitter`ëŠ” ì„¤ì •í•œ seperatorë¥¼ ê¸°ì¤€ìœ¼ë¡œ í…ìŠ¤íŠ¸ë¥¼ ë¶„í• í•œë‹¤. seperatorì˜ ê¸°ë³¸ê°’ì€ `\n\n`, ì¦‰ line breakë¡œ ë¬¸ë‹¨ì„ ë‚˜ëˆ„ëŠ” ê²ƒìœ¼ë¡œ ë¶„í• í•œë‹¤ëŠ” ì˜ë¯¸ë¡œ ì¶”ì •ëœë‹¤. ì´ ê¸°ì¤€ìœ¼ë¡œ ìš°ë¦¬ê°€ ê°€ì§„ Document ì¤‘ 3ë²ˆì§¸ ê°ì²´, ì¦‰ 3ë²ˆì§¸ í˜ì´ì§€ë¥¼ ë¶„í• í•´ë³´ì/

`CharacterTextSplitter`ê°ì²´ë¥¼ ë§Œë“¤ ë•Œ ë‹¤ìŒê³¼ ê°™ì€ ì¸ìë¥¼ ë„£ì–´ ì„¸íŒ…í•´ì£¼ì–´ì•¼ í•œë‹¤.(ì¶œì²˜: í…Œë””ë…¸íŠ¸, [<ë­ì²´ì¸ ë…¸íŠ¸>](https://code.visualstudio.com/docs/editor/codebasics) )
* separator ë§¤ê°œë³€ìˆ˜ë¡œ ë¶„í• í•  ê¸°ì¤€ì„ ì„¤ì •í•©ë‹ˆë‹¤. ê¸°ë³¸ ê°’ì€ "\n\n" ì…ë‹ˆë‹¤.
* chunk_size ë§¤ê°œë³€ìˆ˜ë¥¼ 250 ìœ¼ë¡œ ì„¤ì •í•˜ì—¬ ê° ì²­í¬ì˜ ìµœëŒ€ í¬ê¸°ë¥¼ 250ìë¡œ ì œí•œí•©ë‹ˆë‹¤.
* chunk_overlap ë§¤ê°œë³€ìˆ˜ë¥¼ 50ìœ¼ë¡œ ì„¤ì •í•˜ì—¬ ì¸ì ‘í•œ ì²­í¬ ê°„ì— 50ìì˜ ì¤‘ë³µì„ í—ˆìš©í•©ë‹ˆë‹¤.
* length_function ë§¤ê°œë³€ìˆ˜ë¥¼ lenìœ¼ë¡œ ì„¤ì •í•˜ì—¬ í…ìŠ¤íŠ¸ì˜ ê¸¸ì´ë¥¼ ê³„ì‚°í•˜ëŠ” í•¨ìˆ˜ë¥¼ ì§€ì •í•©ë‹ˆë‹¤.
* is_separator_regex ë§¤ê°œë³€ìˆ˜ë¥¼ Falseë¡œ ì„¤ì •í•˜ì—¬ separatorë¥¼ ì •ê·œì‹ì´ ì•„ë‹Œ ì¼ë°˜ ë¬¸ìì—´ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤.


```python
from langchain.text_splitter import CharacterTextSplitter 

target_doc = docs[1] ## ëŒ€ìƒ ë¬¸ì„œ í™•ì¸
print("â— Target Documnet Content\n")
print(target_doc.page_content)

spliter = CharacterTextSplitter(
    # separator="\n\n",
    chunk_size = 50,
    chunk_overlap = 20,
    length_function = len,
    is_separator_regex=False,
)

result_docs = spliter.create_documents(
    texts = [target_doc.page_content],
    metadatas= [target_doc.metadata],
)
print("\n\nâ— Result:\n")
check_docs(result_docs)
```


    â— Target Documnet Content
    
    In this study, we present a novel framework for indoor element classification and representation in vector format. Unlike traditional methods that rely on image-based learning frameworks for pixel segmentation, our approach converts floor plan images into vector data and utilizes a graph neural network for analysis. The framework consists of three main steps: image pre-processing and vectorization, conversion to a region adjacency graph, and application of the graph neural network for indoor element classification.
    
    Our method accurately classifies indoor elements such as walls, doors, symbols, rooms, and corridors, while also detecting element shapes with a high F1 score of 95%. We also introduce a new graph neural network model that considers the distance between nodes, which is crucial for spatial network data analysis.
    
    The proposed framework aims to address the limitations of existing methods by preserving the original shapes of indoor elements during classification. By leveraging a graph neural network, we can classify basic indoor elements and symbols, along with space elements, without losing their shape and spatial characteristics.
    
    Furthermore, we introduce the Distance-Weighted Graph Neural Network (DWGNN), which assigns attention values to neighboring nodes based on their distance from a target node. This approach enhances the model's performance in spatial network analysis by considering the spatial relationships between nodes.
    
    In the following sections, we discuss the limitations of previous research on floor plan analysis and propose our framework for indoor element classification using a graph neural network. We then present the results of our analysis on multiple datasets and discuss potential issues and areas for further research.
    
    Overall, our framework offers a comprehensive solution for indoor element classification and representation in vector format, addressing the challenges faced by traditional methods. By combining image pre-processing, graph analysis, and machine learning techniques, we achieve high accuracy in indoor element classification while preserving the original shapes and spatial features of indoor elements.

    â–¶ï¸ No. of Documents: 6 
    
    â–¶ï¸ Contents
    * Doc 0: In this study, we present a novel framework for indoor element classification and representation in ...
    â€» Metadata: {}
    
    * Doc 1: Our method accurately classifies indoor elements such as walls, doors, symbols, rooms, and corridors...
    â€» Metadata: {}
    
    * Doc 2: The proposed framework aims to address the limitations of existing methods by preserving the origina...
    â€» Metadata: {}
    
    * Doc 3: Furthermore, we introduce the Distance-Weighted Graph Neural Network (DWGNN), which assigns attentio...
    â€» Metadata: {}
    
    * Doc 4: In the following sections, we discuss the limitations of previous research on floor plan analysis an...
    â€» Metadata: {}
    
    ...
    
    * Doc 5: Overall, our framework offers a comprehensive solution for indoor element classification and represe...
    â€» Metadata: {}


â€‹    

ìš°ë¦¬ëŠ” ì²­í¬ ì‚¬ì´ì¦ˆë¥¼ 50ìœ¼ë¡œ ê½¤ ì‘ê²Œ ì¤¬ì§€ë§Œ ê²°ê³¼ëŠ” ëª‡ê°œì˜ Documentë“¤ë¡œë§Œ ë°˜í™˜ëë‹¤. ê·¸ ì´ìœ ëŠ” ëŒ€ìƒ Documentë¥¼ ë‘ê°œì˜ ì¤„ë°”ê¿ˆ ë¬¸ì(line break, `\n\n`)ë¡œë§Œ ë¶„í•  í–ˆê¸° ë•Œë¬¸ì´ë‹¤. ê·¸ë ‡ë‹¤ë©´ ë§ˆì¹¨í‘œë¥¼ seperatorë¡œ ì„¤ì •í•´ë³´ì.


```python
spliter = CharacterTextSplitter(
    separator=".",
    chunk_size = 50,
    chunk_overlap = 20,
    length_function = len,
    is_separator_regex=False,
)

result_docs = spliter.create_documents(
    texts = [target_doc.page_content],
    metadatas= [target_doc.metadata],
)
print("\n\nâ— Result:\n")
check_docs(result_docs)
```

    â–¶ï¸ No. of Documents: 13 
    
    â–¶ï¸ Contents
    * Doc 0: In this study, we present a novel framework for indoor element classification and representation in ...
    â€» Metadata: {}
    
    * Doc 1: Unlike traditional methods that rely on image-based learning frameworks for pixel segmentation, our ...
    â€» Metadata: {}
    
    * Doc 2: The framework consists of three main steps: image pre-processing and vectorization, conversion to a ...
    â€» Metadata: {}
    
    * Doc 3: Our method accurately classifies indoor elements such as walls, doors, symbols, rooms, and corridors...
    â€» Metadata: {}
    
    * Doc 4: We also introduce a new graph neural network model that considers the distance between nodes, which ...
    â€» Metadata: {}
    
    ...
    
    * Doc 12: By combining image pre-processing, graph analysis, and machine learning techniques, we achieve high ...
    â€» Metadata: {}

ë¬¸ì¥ ë‹¨ìœ„ë¡œ êµ¬ë¶„ì´ ëìœ¼ë‚˜ ì•Œê³ ë¦¬ì¦˜ ì„¤ëª… í…ìŠ¤íŠ¸, ì°¸ê³  ë¬¸í—Œ ë“±ì—ì„œë„ ë§ˆì¹¨í‘œê°€ ë‚˜ì˜¬ ìˆ˜ ìˆê¸°ì— ëª©ì ì— ë§ê²Œ seperatorë¥¼ ì„¤ì •í•´ì•¼í•œë‹¤.

TextSpliterëŠ” ê¸°ë³¸ì ì¸ CharacterTextSpliterë§ê³ ë„ RecursiveCharacterTextSpliter(ì¬ê·€ì  ë¬¸ì í…ìŠ¤íŠ¸ ë¶„í• ), TokenTextSpliter(í† í° í…ìŠ¤íŠ¸ ë¶„í• ) ë“± ë¶„í•  ë°©ë²•ì— ë”°ë¼ ë‹¤ì–‘í•˜ê²Œ ë‚˜ë‰˜ê¸°ë„ í•˜ë©° ì†ŒìŠ¤ ì½”ë“œ ë¶„í• , JSONë¶„í• , Markdowní…ìŠ¤íŠ¸ ë¶„í• , HTMLë¶„í•  ë“± í…ìŠ¤íŠ¸ í¬ë§·, ìš©ë„ì— ë”°ë¥¸ ì „ë¬¸ì ì¸ Spliterë¡œ ë‚˜ë‰˜ê¸°ë„ í•œë‹¤. [ë­ì²´ì¸ API Doc](https://api.python.langchain.com/en/stable/langchain_api_reference.html#module-langchain.text_splitter)ì˜ langchain.text_splitter ì—ì„œ ë¦¬ìŠ¤íŠ¸ë¥¼ í™•ì¸ ê°€ëŠ¥í•˜ë‹¤.

ê¸°ë³¸ì ì¸ TextSpliterì˜ ì‘ë™ ë°©ì‹ì„ ì•Œì•„ë´¤ìœ¼ë‹ˆ ë°”ë¡œ ë³¸ í”„ë¡œì íŠ¸ì˜ ëª©ì ì— ë§ëŠ” TextSpliterë¥¼ ì‚¬ìš©í•´ë³´ì. 



## SemanticChunker

Semantic ChunkerëŠ” ì˜ë¯¸ë¡ ì  ìœ ì‚¬ë„ë¡œ textë¥¼ ë‚˜ëˆ„ëŠ” ê²ƒì´ë‹¤. ë¬¸ì„œ ì•ˆì—ì„œ ë¬¸ì¥ì„ ë‚˜ëˆ„ë©´ ê°€ê¹Œìš´ ë¬¸ì¥ ë¼ë¦¬ëŠ” ì˜ë¯¸ë¡ ì ìœ¼ë¡œ ìœ ì‚¬í•  ê²ƒì´ë‹¤. ê°„ë‹¨í•œ ê·œì¹™ìœ¼ë¡œ ë¬¸ì¥ì„ ë‹¨ìˆœí•˜ê²Œ ë‚˜ëˆˆëŠ” ê²ƒë³´ë‹¤ ê±°ë¦¬ìƒ ê°€ê¹Œìš´ ë¬¸ì¥ì´ ì˜ë¯¸ë¡ ì ìœ¼ë¡œë„ ë¹„ìŠ·í•˜ë‹¤ë©´ í•˜ë‚˜ì˜ Documentë¡œ ë¬¶ì–´ì£¼ëŠ” ë°©ë²•ì´ ìì—°ìŠ¤ëŸ½ë‹¤. `SemanticChunker`ëŠ” ì´ ë°©ë²•ì„ ì ìš©í•œ splitterë¡œ ë¶„í•  ë°©ë²•ì€ ë‹¤ìŒê³¼ ê°™ë‹¤.
1. í…ìŠ¤íŠ¸ë¥¼ ë¶„í• í•  ë•Œ ë¬¸ì¥ ë‹¨ìœ„ë¡œ ìš°ì„  ë¶„í• í•œ í›„ ê° ë¬¸ì¥ì„ ì• ë’¤ ë¬¸ì¥ê³¼ í•¨ê»˜ 3ê°œë¡œ ì´ì–´ì¤€ë‹¤(buffer_sizeê°€ 1ì¼ ê²½ìš°; ìœˆë„ìš° = 3).
    * ì™„ê²°ëœ ë¬¸ì¥ìœ¼ë¡œ ì¸ì‹í•˜ëŠ” ê¸°ì¤€ì€ ".", "?", "!"ì˜ ì¶œí˜„ ì—¬ë¶€ì´ë‹¤. ì •ê·œ í‘œí˜„ì‹ì„ ì‚¬ìš©í•œ seperatorë¡œ í…ìŠ¤íŠ¸ë¥¼ ë¶„í• í•œë‹¤.
2. ì•ë’¤ ë¬¸ì¥ë“¤ê³¼ ì´ì–´ì§„ ê° ë¬¸ì¥ë“¤ì„ ì„ë² ë”© ëª¨ë¸ì— ë„£ì–´ ì„ë² ë”© ë²¡í„°ë¥¼ ì–»ëŠ”ë‹¤.

3. ê° ë²¡í„°ë“¤ì„ ë°”ë¡œ ë‹¤ìŒ ë²¡í„°ì™€ì˜ distanceë¥¼ êµ¬í•œë‹¤.
    * [ì›ë¬¸](https://github.com/FullStackRetrieval-com/RetrievalTutorials/blob/8a30b5710b3dd99ef2239fb60c7b54bc38d3613d/tutorials/LevelsOfTextSplitting/5_Levels_Of_Text_Splitting.ipynb)ì—ì„  cosine distanceë¥¼ ì‚¬ìš©í–ˆë‹¤.
4. êµ¬í•´ì§„ ë²¡í„°ë“¤ì˜ ë¶„í¬ì—ì„œ ë‹¤ì–‘í•œ ì „ëµ/í†µê³„ ê°’(percentile, standard deviation ë“±)ì„ ê¸°ì¤€ìœ¼ë¡œ ë¶„í• í•œ ë’¤ ë§Œì¡±í•˜ëŠ” ê²½ìš°ì˜ indexë¥¼ break pointë¡œ ì§€ì •, ì „ì²´ ê·¸ë£¹ì„ ë¶„í• í•˜ì—¬ ê°™ì€ ê·¸ë£¹ì˜ ë¬¸ì¥ë“¤ì„ ê°™ì€ chuckë¡œ ë¬¶ëŠ”ë‹¤.
    * ì˜ˆë¥¼ ë“¤ì–´ êµ¬í•´ì§„ ë²¡í„° ë¶„í¬ì—ì„œ ì „ëµì„ percentileë¡œ, threshold ê°’ì„ .95ë¡œ ì„¤ì •í•œë‹¤ë©´ distanceê°€ ìƒìœ„ 95% ì•ˆì— ë“œëŠ” ê²½ìš°ë¥¼ break pointë¡œ ì •í•´ ê·¸ë£¹ì„ ë‚˜ëˆ„ê³  ê° ê·¸ë£¹ì— ì†í•œ ë¬¸ì¥ë“¤ì„ ê°™ì€ chunkì— ë„£ëŠ” ê²ƒì´ë‹¤.


```python
from langchain_experimental.text_splitter import SemanticChunker
from langchain_openai import OpenAIEmbeddings
```

SemanticChunker ëŠ” langchain ì •ì‹ ë²„ì „ì´ ì•„ë‹Œ langchain_experimentalì— ìˆê¸°ì— `pip install langchain_experimental`ë¡œ ë”°ë¡œ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì„¤ì¹˜í•´ì¤˜ì•¼ í•œë‹¤.
ê·¸ë¦¬ê³  chunking ê³¼ì • ì¤‘ ì„ë² ë”© ë²¡í„°ë¡œ ë³€í™˜í•˜ëŠ” ë‹¨ê³„ê°€ ìˆê¸°ì— OpenAIEmbeddingsë¥¼ ì„í¬íŠ¸í•´ ì‚¬ìš©í•œë‹¤.


```python
semanticChunker_percentile = SemanticChunker(
    embeddings= OpenAIEmbeddings(),
    buffer_size = 1, # ê° ë¬¸ìì˜ ì´ì „/ì´í›„ í•œ ë¬¸ì¥ë“¤ë§Œ ë¶™ì—¬ ì„ë² ë”© ë²¡í„°ë¥¼ ìƒì„±í•œë‹¤. ì¦‰ ì–‘ì˜†ìœ¼ë¡œ sizeê°€ 1ì¸ ìœˆë„ìš°ë¥¼ ì„¤ì •í•˜ëŠ” ê²ƒ
    breakpoint_threshold_type='percentile',
    breakpoint_threshold_amount=75, # ë°±ë¶„ìœ„ìˆ˜ë¥¼ ìŠµê´€ì ìœ¼ë¡œ ì†Œìˆ˜ì ìœ¼ë¡œ ì ëŠ” ë²„ë¦‡ì´ ìˆëŠ”ë° ë°±ë¶„ìœ„ìˆ˜ ê·¸ëŒ€ë¡œ ì ì–´ì¤˜ì•¼í•œë‹¤. ë§Œì•½ 90% ì´ìƒì˜ ê°’ì—ì„œë§Œ ë‚˜ëˆ„ê³  ì‹¶ì„ ê²½ìš° .9ê°€ ì•„ë‹Œ 90ìœ¼ë¡œ ë„£ì–´ì¤˜ì•¼ í•œë‹¤.
) 

result_docs = semanticChunker_percentile.create_documents(
    texts = [target_doc.page_content],
    metadatas= [target_doc.metadata],
)
print("\n\nâ— Result:\n")
check_docs(result_docs, show_len = 100)
```

    â–¶ï¸ No. of Documents: 4 
    
    â–¶ï¸ Contents
    * Doc 0: In this study, we present a novel framework for indoor element classification and representation in ...
    â€» Metadata: {}
    
    * Doc 1: The proposed framework aims to address the limitations of existing methods by preserving the origina...
    â€» Metadata: {}
    
    * Doc 2: In the following sections, we discuss the limitations of previous research on floor plan analysis an...
    â€» Metadata: {}
    
    ...
    
    * Doc 3: Overall, our framework offers a comprehensive solution for indoor element classification and represe...
    â€» Metadata: {}


â€‹    

ë°±ë¶„ìœ„ìˆ˜ë¥¼ 75%ë¡œ ì„¤ì •í–ˆì„ ê²½ìš° ì´ 13ê°œì˜ ì„ë² ë”©ëœ ë¬¸ì„œë“¤ì´ ê°ìì˜ `distance` ê°’ ë¶„í¬ì— ë”°ë¼ 3ê°œ(`floor(13 - (13*.75)) = 3`)ì˜ ìƒìœ„ ì¸ë±ìŠ¤ë¡œ ë‚˜ëˆ ì§€ë¯€ë¡œ ì „ì²´ ë¬¸ì¥ setì„ 4ê°œì˜ Documentsë¡œ í•©ì³ì ¸ ë°˜í™˜ëœë‹¤. ê° DocumentëŠ” ì˜ë¯¸ì ìœ¼ë¡œ ë¹„ìŠ·í•œ ë¬¸ì¥ë“¤ ê°„ì˜ chunkë¡œ êµ¬ì„±ëœë‹¤.


```python
semanticChunker_stdv = SemanticChunker(
    embeddings= OpenAIEmbeddings(),
    buffer_size = 2,
    breakpoint_threshold_type='standard_deviation',
    breakpoint_threshold_amount=1.25, # ì •ê·œ ë¶„í¬ ìƒ í‘œì¤€í¸ì°¨ ê°’, ì–‘ìˆ˜ ë¶€ë¶„ë§Œ í•´ë‹¹ëœë‹¤.
) 

result_docs = semanticChunker_stdv.create_documents(
    texts = [target_doc.page_content],
    metadatas= [target_doc.metadata],
)
print("\n\nâ— Result:\n")
check_docs(result_docs, show_len = 100)
```

    â–¶ï¸ No. of Documents: 3 
    
    â–¶ï¸ Contents
    * Doc 0: In this study, we present a novel framework for indoor element classification and representation in ...
    â€» Metadata: {}
    
    * Doc 1: The proposed framework aims to address the limitations of existing methods by preserving the origina...
    â€» Metadata: {}
    
    ...
    
    * Doc 2: We then present the results of our analysis on multiple datasets and discuss potential issues and ar...
    â€» Metadata: {}


â€‹    

í‘œì¤€í¸ì°¨ë¡œ ì„¤ì •í•œ ê²½ìš° 1.25ëŠ” ì •ê·œ ë¶„í¬ ìƒ ì•½ 79%ë¥¼ ì»¤ë²„í•˜ë©° í‰ê· (0)ë³´ë‹¤ ì‘ì€ ë²”ìœ„ë¥¼ ë¹¼ë©´ ì „ì²´ì˜ ì•½ 89.5ë¥¼ ì»¤ë²„í•˜ë¯€ë¡œ ë‚˜ë¨¸ì§€(11.5%)ì— í•´ë‹¹í•˜ëŠ” ë†’ì€ distance ê°’ë“¤ë§Œì„ ê¸°ì¤€ìœ¼ë¡œ chunkingëœë‹¤(`ceil(13 - 13*.895) = 2`).

ìµœì¢…ì ìœ¼ë¡œ SemanticChunkerë¥¼ ì‚¬ìš©í•´ ì „ì²´ ë…¼ë¬¸ë“¤ì„ splití•˜ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤(ì„ë² ë”© ê³¼ì • ë•Œë¬¸ì— ì‹œê°„ì´ ê½¤ ì†Œìš”ëœë‹¤).
* ì„ë² ë”© ë²¡í„° ìƒì„± ê¸°ì¤€ ìœˆë„ìš° í¬ê¸° : 2(ê° ë¬¸ì¥ ê¸°ì¤€ ì•ë’¤ 2ê°œ ì´ 5ê°œì˜ ë¬¸ì¥ concat)
* ë¶„í•  ê¸°ì¤€: ë°±ë¶„ìœ„ ìˆ˜ 80% ì´ìƒì˜ distanceë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë¶„í• 

ê·¸ ê²°ê³¼ ì´ 17í˜ì´ì§€(í˜ì´ì§€ë‹¹ í•˜ë‚˜ì˜ Document)ë¡œ êµ¬ì„±ëœ ì˜ˆì‹œ ë…¼ë¬¸ì´ ì´ 74ê°œì˜ Documentsë¡œ ë¶„í• ëë‹¤.

```python
semanticChunker = SemanticChunker(
    embeddings= OpenAIEmbeddings(),
    buffer_size = 2,
    breakpoint_threshold_type='percentile',
    breakpoint_threshold_amount=80, # ë°±ë¶„ìœ„ìˆ˜ë¥¼ ìŠµê´€ì ìœ¼ë¡œ ì†Œìˆ˜ì ìœ¼ë¡œ ì ëŠ” ë²„ë¦‡ì´ ìˆëŠ”ë° ë°±ë¶„ìœ„ìˆ˜ ê·¸ëŒ€ë¡œ ì ì–´ì¤˜ì•¼í•œë‹¤. ë§Œì•½ 90% ì´ìƒì˜ ê°’ì—ì„œë§Œ ë‚˜ëˆ„ê³  ì‹¶ì„ ê²½ìš° .9ê°€ ì•„ë‹Œ 90ìœ¼ë¡œ ë„£ì–´ì¤˜ì•¼ í•œë‹¤.
) 

result_docs = semanticChunker.split_documents(
    documents= docs,
)
print("\n\nâ— Result:\n")
check_docs(result_docs, show_len = 100)
```

    â–¶ï¸ No. of Documents: 74 
    
    â–¶ï¸ Contents
    * Doc 0: Abstract: This paper introduces a novel framework for the classification of indoor elements and thei...
    â€» Metadata: {}
    
    * Doc 1: Digitizing floor plans presents challenges as they are essentially images without explicit object in...
    â€» Metadata: {}
    
    * Doc 2: While heuristic algorithms have shown high accuracy, they are limited to specific drawing styles. To...
    â€» Metadata: {}
    
    * Doc 3: For instance, walls may be represented as line vectors instead of having their own thickness and are...
    â€» Metadata: {}
    
    * Doc 4: In this study, we present a novel framework for indoor element classification and representation in ...
    â€» Metadata: {}
    
    ...
    
    * Doc 73: Wang, M.; Yu, L.; Zheng, D.; Gan, Q.; Gai, Y.; Ye, Z.; Huang, Z. Deep graph library: Towards efficie...
    â€» Metadata: {}


â€‹    

## ë²¡í„° ìŠ¤í† ì–´ì— ì ìš©

ë‹¤ìŒìœ¼ë¡œ ë²¡í„° [ì§€ë‚œ í¬ìŠ¤íŠ¸](https://lymanstudio.github.io/langchain/rag_1_loader_vectorstore/)ì—ì„œ ì§„í–‰í•œ ë²¡í„° ìŠ¤í† ì–´ë¥¼ êµ¬ì„±í•´ì£¼ì—ˆë‹¤.
ë²¡í„° ìŠ¤í† ì–´ëŠ” FAISSë¥¼ ì‚¬ìš©í–ˆë‹¤.


```python
from langchain_core.vectorstores import VectorStore #Vector Storeë“¤ì˜ ë² ì´ìŠ¤ í´ë˜ìŠ¤
from langchain_core.documents import Document
from typing import List
from langchain_community.vectorstores import chroma, faiss

open_ai_embedding_model = OpenAIEmbeddings()

def get_vector_store(vector_store_model:VectorStore, documents:List[Document], embedding_model):
    return vector_store_model.from_documents(embedding = embedding_model, documents = documents)

vector_store = get_vector_store(
    vector_store_model = faiss.FAISS, 
    documents = result_docs, 
    embedding_model = open_ai_embedding_model
)

```


```python
import numpy as np
import math 

def print_retrieved_doc(rank, idx, score, vs, doc):
    return f"""
- K: {rank}
- index: {idx}
- similiarity: {round(score*100, 1)}%
- docstore_id: {vs.index_to_docstore_id[idx]}
- page_content: {doc.page_content}
    """

def get_doc(idx, vs = vector_store):
    return vs.docstore.search(vs.index_to_docstore_id[idx])

get_relevance = lambda x: 1.0 - x / math.sqrt(2)

user_input_query = 'How to make a Region Adjacency Graph(RAG)?'
user_input_q_vector = np.array([open_ai_embedding_model.embed_query(user_input_query)], dtype=np.float32)

results = vector_store.index.search(x = user_input_q_vector, k = 5)
print(results) #(score, index)

scores, doc_ids = results[0][0], results[1][0]

for doc_rank, (s, idx, doc) in enumerate(zip(map(get_relevance, scores), doc_ids, map(get_doc, doc_ids))):
    print(print_retrieved_doc(doc_rank, idx, s, vector_store, doc))

```

    (array([[0.25520685, 0.25867853, 0.26270026, 0.26567715, 0.33161542]],
          dtype=float32), array([[13, 22, 19, 28, 23]], dtype=int64))
    
    - K: 0
    - index: 13
    - similiarity: 82.0%
    - docstore_id: f4959094-d2a5-434b-bf07-d5c7ab1673d0
    - page_content: The proposed framework involves preprocessing the raster floor plan image to obtain a binarized image for vectorization. Closed regions in the image are transformed into polygons post-vectorization, which are then converted into a region adjacency graph (RAG) based on their adjacent relationships. The RAG is utilized to train a GNN model, resulting in a set of polygons with different classes as the final output. The overview of this framework is depicted in Figure 1.
        
    
    - K: 1
    - index: 22
    - similiarity: 81.7%
    - docstore_id: fe6c0afb-e88e-47d6-94d5-35124b384454
    - page_content: It is important to buffer the polygons by the thickness of the pixel lines surrounding them to ensure that regions occupied by pixel lines are not included in the polygons. This buffering process is essential for maintaining adjacency relationships between polygons. Once the polygons are buffered, they are converted into a Region Adjacency Graph (RAG) in order to extract features. The RAG conversion process involves creating an empty graph and adding the centroid of each polygon as a node.
        
    
    - K: 2
    - index: 19
    - similiarity: 81.4%
    - docstore_id: f2f04b63-6ea7-4ee6-a47e-d07fddec7874
    - page_content: It is essential to buffer the polygons by the thickness of the pixel lines surrounding them to ensure that regions occupied by pixel lines are not included in the polygons. This buffering process is crucial for maintaining adjacency relationships between polygons, with the buffering distance parameter selected as half the thickness of the pixel line. The next stage involves converting the polygons into a Region Adjacency Graph (RAG) and extracting features. The RAG conversion process begins by creating an empty graph and adding the centroid of each polygon as a node. To determine the edges of the graph, each polygon intersects with other polygons in the set.
        
    
    - K: 3
    - index: 28
    - similiarity: 81.2%
    - docstore_id: 2967f351-6a66-4f47-972d-ceaa58c621bc
    - page_content: It is important to buffer the polygons by the thickness of the pixel lines surrounding them to ensure that regions occupied by pixel lines are not included in the polygons. This buffering process is essential for maintaining adjacency relationships between polygons. Once the polygons are buffered, they are converted into a Region Adjacency Graph (RAG) in order to extract features. The RAG conversion process involves creating an empty graph and adding the centroid of each polygon as a node. The edges of the graph are determined by the intersections of each polygon with others in the set. To optimize this process and reduce complexity, an STRtree spatial indexing algorithm is utilized, which significantly improves the efficiency of the conversion process.
        
    
    - K: 4
    - index: 23
    - similiarity: 76.6%
    - docstore_id: 128543e5-a511-4467-8cae-6e32f1f39b4d
    - page_content: The edges of the graph are determined by the intersections of each polygon with others in the set. To optimize this process and reduce complexity, an STRtree spatial indexing algorithm is utilized, which significantly improves the efficiency of the conversion process. Overall, the framework presented in this study offers a comprehensive approach to floor plan analysis by incorporating Graph Neural Networks (GNNs) for accurate indoor element classification and representation in vector format. This innovative approach considers spatial relationships, shape features, and inductive learning techniques to enhance the classification of indoor elements while preserving their original characteristics. Algorithm 1 outlines the process of RAG conversion, where a polygon set P and a minimum area parameter m are used to create a floor plan graph G.


â€‹    

ê°„ë‹¨í•˜ê²Œ retrieverë¡œ ì§ˆë¬¸í•˜ë©´ ë‹¤ìŒê³¼ ê°™ì´ ë‚˜ì˜¨ë‹¤.


```python
retriever = vector_store.as_retriever()

retriever.get_relevant_documents(query = 'What are the key features of DWGNN?')
```




    [Document(page_content="The update process of DWGNN involves aggregating information from neighboring nodes using an aggregation function and updating the target node's embedding vector based on this aggregated information. By incorporating distance weights in the aggregation and update processes, DWGNN effectively captures spatial relationships within the network, thereby enhancing the accuracy of node classification tasks. This innovative model fills a significant gap in existing GNN approaches by specifically addressing the unique characteristics of spatial networks and edge features. In conclusion, the DWGNN model presents a novel and effective approach to incorporating edge features in spatial networks, resulting in improved performance in node classification tasks. Experimental results on various floor plan benchmarks and a data-augmented dataset demonstrate the efficacy of the proposed framework. DWGNN emerges as a promising GNN model for analyzing spatial graphs where weights play a critical role."),
     Document(page_content="The update process of DWGNN involves aggregating information from neighboring nodes using an aggregation function and updating the target node's embedding vector based on this aggregated information. By considering distance weights in the aggregation and update processes, DWGNN effectively captures spatial relationships within the network, thereby improving the accuracy of node classification tasks. This innovative model fills a significant gap in existing GNN approaches by specifically addressing the unique characteristics of spatial networks and edge features. In conclusion, the DWGNN model presents a novel and effective approach to incorporating edge features in spatial networks, leading to improved performance in node classification tasks. The results of experiments conducted on various floor plan benchmarks, along with a data-augmented dataset, demonstrate the effectiveness of the proposed framework. DWGNN emerges as a promising GNN model for analyzing spatial graphs where weights play a crucial role."),
     Document(page_content='Glimmer et al. proposed a model that integrates edge features into the message-passing process, but its generality limits its effectiveness. To bridge this gap, we introduce the Distance-Weighted Graph Neural Network (DWGNN), an inductive learning-based GNN model that enhances the representation of spatial networks by incorporating an edge feature mechanism in the message-passing process. DWGNN, based on GraphSAGE, uses the distance between nodes as a one-dimensional weight value that influences the aggregation and update processes. By assigning attention values based on relative distance from the target node, DWGNN enables more context-aware updates.'),
     Document(page_content='While Glimmer et al. proposed a model that integrates edge features into the message-passing process, its generality limits its effectiveness. To address this gap, we introduce the Distance-Weighted Graph Neural Network (DWGNN), an inductive learning-based GNN model that enhances spatial network representation by incorporating an edge feature mechanism in the message-passing process. Based on GraphSAGE, DWGNN uses the distance between nodes as a one-dimensional weight value to influence aggregation and update processes. By assigning attention values based on relative distance from the target node, DWGNN facilitates more context-aware updates.')]

ë§ˆì§€ë§‰ìœ¼ë¡œ ìœ„ ëª¨ë¸ì„ ë¡œì»¬ í´ë”ì— ì €ì¥í•´ì£¼ì. ë‹¤ìŒ í¬ìŠ¤íŠ¸ì—ì„œ êµ¬ì„± ëœ ë²¡í„° ìŠ¤í† ì–´ë§Œ ë¶ˆëŸ¬ì™€ ìµœì¢… ëª¨ë¸ì„ êµ¬ì„±í•  ì˜ˆì •ì´ë‹¤.

```py
vector_store.save_local(os.path.join(base_dir, 'models/vector_stores/pdf_rag_faiss_index'))
```



ë‹¤ìŒ í¬ìŠ¤íŠ¸ì—ì„  ì´ë ‡ê²Œ êµ¬ì„±ëœ ë²¡í„° ìŠ¤í† ì–´ë¥¼ í™œìš©í•´ ì‚¬ìš©ì ì…ë ¥ì— ëŒ€í•´ ê²€ìƒ‰í•œ ë¬¸ì„œë“¤ì„ ì»¨í…ìŠ¤íŠ¸ë¡œ ì£¼ì–´ LLM ëª¨ë¸ì„ í†µí•´ ê²°ê³¼ë¥¼ ë„ì¶œí•˜ê³  ìœ ì €ì—ê²Œ ìì—°ì–´ë¡œ ìš”ì•½í•´ ì„¤ëª…í•˜ëŠ” Chainì„ êµ¬ì„±í•´ ë³¼ ì˜ˆì •ì´ë‹¤.





> ì°¸ê³  ë¬¸ì„œ ë° ë§í¬

- ì˜ˆì œ ë…¼ë¬¸ ë°ì´í„°: Song J, Yu K. Framework for Indoor Elements Classification via Inductive Learning on Floor Plan Graphs. *ISPRS International Journal of Geo-Information*. 2021; 10(2):97. [[ğŸ”—](Song J, Yu K. Framework for Indoor Elements Classification via Inductive Learning on Floor Plan Graphs. *ISPRS International Journal of Geo-Information*. 2021; 10(2):97. https://doi.org/10.3390/ijgi10020097)]
- LangChain API reference [[ğŸ”—](https://api.python.langchain.com/en/stable/langchain_api_reference.html#module-langchain.text_splitter)]
- 5 Level of Text Splitting[[ğŸ”—](https://github.com/FullStackRetrieval-com/RetrievalTutorials/blob/8a30b5710b3dd99ef2239fb60c7b54bc38d3613d/tutorials/LevelsOfTextSplitting/5_Levels_Of_Text_Splitting.ipynb)]
